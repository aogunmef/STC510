{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\deolu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\deolu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "For the Essentials Badge, I'm going to do two different approaches. Both will involve deep learning, however one will try to limit the alteration to the original input data as much as possible in order to do a straight comparison. The second method will be a more involved approach at pre-proccessing the data before feeding it into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'for the last 8 years of his life, galileo was under house arrest for espousing this man's theory'\",\n",
       " \"'no. 2: 1912 olympian; football star at carlisle indian school; 6 mlb seasons with the reds, giants & braves'\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('jeopardy.json')\n",
    "rawData = json.load(file)\n",
    "\n",
    "corpus = [entry['question'].lower() for entry in rawData if entry['value']!=None]\n",
    "corpus[0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213296,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worth = [1 if int(item['value'].strip('$').replace(',','')) >= 500 else 0 for item in rawData if item['value']!=None]\n",
    "# worth = [int(num['value'].strip('$')) for num in worth]\n",
    "\n",
    "worth = np.array(worth)\n",
    "worth.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159972,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(corpus, worth, random_state=1)\n",
    "# print(\"Training Size: \" + str(len(train_x)))\n",
    "np.array(train_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(n_features=2**10)\n",
    "# vectorizer = CountVectorizer(stop_words='english')\n",
    "trainVectorized = vectorizer.fit_transform(train_x)\n",
    "testVectorized = vectorizer.transform(test_x)\n",
    "trainVectorized = np.array(trainVectorized.toarray())\n",
    "testVectorized = np.array(testVectorized.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.6748 - accuracy: 0.5771\n",
      "Epoch 2/20\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.6650 - accuracy: 0.5956\n",
      "Epoch 3/20\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.6378 - accuracy: 0.6355\n",
      "Epoch 4/20\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.5867 - accuracy: 0.6921\n",
      "Epoch 5/20\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.5237 - accuracy: 0.7439\n",
      "Epoch 6/20\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.4642 - accuracy: 0.7848\n",
      "Epoch 7/20\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.4114 - accuracy: 0.8160\n",
      "Epoch 8/20\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.3661 - accuracy: 0.8423\n",
      "Epoch 9/20\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.3269 - accuracy: 0.8640\n",
      "Epoch 10/20\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.2923 - accuracy: 0.8824\n",
      "Epoch 11/20\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.2615 - accuracy: 0.8975\n",
      "Epoch 12/20\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.2342 - accuracy: 0.9099\n",
      "Epoch 13/20\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.2105 - accuracy: 0.9215\n",
      "Epoch 14/20\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.1889 - accuracy: 0.9317\n",
      "Epoch 15/20\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.1697 - accuracy: 0.9404\n",
      "Epoch 16/20\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.1523 - accuracy: 0.9477\n",
      "Epoch 17/20\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.1369 - accuracy: 0.9547\n",
      "Epoch 18/20\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.1238 - accuracy: 0.9604\n",
      "Epoch 19/20\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.1118 - accuracy: 0.9652\n",
      "Epoch 20/20\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 0.1013 - accuracy: 0.9694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fd6af71f90>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainVectorized, train_y, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1667/1667 - 2s - loss: 2.2073 - accuracy: 0.5321 - 2s/epoch - 1ms/step\n",
      "\n",
      "Test accuracy: 0.5321431159973145\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(testVectorized,  test_y, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Try a Different method\n",
    "\n",
    "For this method, I used a different method where I increased the number of possible values from 2 (\"low\" or \"high\") to 10 (actual values). I simply used 10 different classes for the inputs. You can see how I assigned them in the module below. I then used a different way of measuring error as well. \n",
    "\n",
    "Furthermore, I used a different Vecotrizer. I used a hash vectorizer. This had more to do with limiting the size of the matrix than anything else\n",
    "\n",
    "Overall, this gave me a much better result. I belive this has more to do with the extension of the possible values than the diffrent error metric or the hash vectorizer. Given that these values come from jeopardy, there should be some rationale (common theme) between the questions that can be identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213296, 10)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worthExtra = [int(item['value'].strip('$').replace(',','')) for item in rawData if item['value']!=None]\n",
    "worthConverted = []\n",
    "i = 0\n",
    "j = 0\n",
    "for value in worthExtra:\n",
    "    if(value<=100):\n",
    "        worthConverted.append([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "    elif (value>100 & value <= 200):\n",
    "        worthConverted.append([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "    elif (value==300):\n",
    "        worthConverted.append([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "    elif (value==400):\n",
    "        worthConverted.append([0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
    "    elif (value==500):\n",
    "        worthConverted.append([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
    "    elif (value==600):\n",
    "        worthConverted.append([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
    "    elif (value==700):\n",
    "        worthConverted.append([0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
    "    elif (value==800):\n",
    "        worthConverted.append([0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n",
    "    elif (value==900):\n",
    "        worthConverted.append([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
    "    elif (value>=1000):\n",
    "        worthConverted.append([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
    "    else:\n",
    "        j = j + 1\n",
    "    i = i + 1\n",
    "    # if (i == len(worthExtra)):\n",
    "        # print(j)\n",
    "worthConverted = np.array(worthConverted)\n",
    "# len(worthConverted)\n",
    "worthConverted.shape\n",
    "# len(worthExtra)\n",
    "# min(worthExtra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213296,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x2, test_x2, train_y2, test_y2 = train_test_split(corpus, worthConverted, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 0.0081 - accuracy: 0.9577\n",
      "Epoch 2/20\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 0.0081 - accuracy: 0.9577\n",
      "Epoch 3/20\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 0.0080 - accuracy: 0.9577\n",
      "Epoch 4/20\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 0.0079 - accuracy: 0.9577\n",
      "Epoch 5/20\n",
      "5000/5000 [==============================] - 9s 2ms/step - loss: 0.0078 - accuracy: 0.9577\n",
      "Epoch 6/20\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 0.0076 - accuracy: 0.9578\n",
      "Epoch 7/20\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 0.0074 - accuracy: 0.9581\n",
      "Epoch 8/20\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 0.0072 - accuracy: 0.9587\n",
      "Epoch 9/20\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 0.0069 - accuracy: 0.9595\n",
      "Epoch 10/20\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 0.0068 - accuracy: 0.9604\n",
      "Epoch 11/20\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 0.0066 - accuracy: 0.9614\n",
      "Epoch 12/20\n",
      "5000/5000 [==============================] - 9s 2ms/step - loss: 0.0064 - accuracy: 0.9626\n",
      "Epoch 13/20\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 0.0062 - accuracy: 0.9636\n",
      "Epoch 14/20\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 0.0061 - accuracy: 0.9647\n",
      "Epoch 15/20\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 0.0059 - accuracy: 0.9657\n",
      "Epoch 16/20\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 0.0058 - accuracy: 0.9666\n",
      "Epoch 17/20\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 0.0057 - accuracy: 0.9674\n",
      "Epoch 18/20\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 0.0055 - accuracy: 0.9684\n",
      "Epoch 19/20\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 0.0054 - accuracy: 0.9691\n",
      "Epoch 20/20\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0053 - accuracy: 0.9698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fd8eabd660>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(trainVectorized, train_y2, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1667/1667 - 2s - loss: 0.0106 - accuracy: 0.9454 - 2s/epoch - 1ms/step\n",
      "\n",
      "Test accuracy: 0.9454467296600342\n"
     ]
    }
   ],
   "source": [
    "test_loss2, test_acc2 = model2.evaluate(testVectorized,  test_y2, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "868cd44cba082c7532b187f50c3e99f57881545b38974f3fe82673312c7f1a4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
